# ──────────────── Ollama LLM ──────────────── #
# Make sure Ollama is running: ollama serve
# Pull the model: ollama pull llama3.2

# Optional: override Ollama URL or model
# OLLAMA_URL=http://localhost:11434/api/generate
# OLLAMA_MODEL=llama3.2

# Optional: custom wake word
# WAKE_WORD=hey computer
